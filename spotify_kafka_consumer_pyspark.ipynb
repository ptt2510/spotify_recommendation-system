{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182d8499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 23:33:05 WARN Utils: Your hostname, ptt-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.93.135 instead (on interface ens33)\n",
      "24/03/15 23:33:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ptt/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ptt/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ptt/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9b9ee456-7f49-43c8-a597-b24c4039f142;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      ":: resolution report :: resolve 1604ms :: artifacts dl 85ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   1   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9b9ee456-7f49-43c8-a597-b24c4039f142\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/16ms)\n",
      "24/03/15 23:33:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 23:33:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/15 23:33:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.93.135:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ec50c0d0810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.0.1' # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0' #your kafka version\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"kafka-example\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be54f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "import random\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "kafka_topic_name = \"songTopic10\"\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "songs_df = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers).option(\"subscribe\", kafka_topic_name).option(\"startingOffsets\", \"earliest\").load()\n",
    "songs_df1 = songs_df.selectExpr(\"cast(value as string) as value\")\n",
    "songs_df2=songs_df1.select(\"value\", f.translate(f.col(\"value\"), '\"', \"\").alias(\"values\"))\n",
    "songs_df3 = songs_df2.withColumn('order_id', split(songs_df2['values'], '~').getItem(0).cast(\"int\"))\\\n",
    "                .withColumn(\"id\", split(songs_df2['values'], '~').getItem(1).cast(\"string\"))\\\n",
    "                .withColumn(\"name\", split(songs_df2['values'], '~').getItem(2).cast(\"string\"))\\\n",
    "                .withColumn(\"popularity\", split(songs_df2['values'], '~').getItem(3).cast(\"int\"))\\\n",
    "                .withColumn(\"duration_ms\", split(songs_df2['values'], '~').getItem(4).cast(\"double\"))\\\n",
    "                .withColumn(\"explicit\", split(songs_df2['values'], '~').getItem(5).cast(\"int\"))\\\n",
    "                .withColumn(\"artists\", split(songs_df2['values'], '~').getItem(6).cast(\"string\"))\\\n",
    "                .withColumn(\"id_artists\", split(songs_df2['values'], '~').getItem(7).cast(\"string\"))\\\n",
    "                .withColumn(\"release_date\", split(songs_df2['values'], '~').getItem(8).cast(\"string\"))\\\n",
    "                .withColumn(\"danceability\", split(songs_df2['values'], '~').getItem(9).cast(\"double\"))\\\n",
    "                .withColumn(\"energy\", split(songs_df2['values'], '~').getItem(10).cast(\"double\"))\\\n",
    "                .withColumn(\"key\", split(songs_df2['values'], '~').getItem(11).cast(\"int\"))\\\n",
    "                .withColumn(\"loudness\", split(songs_df2['values'], '~').getItem(12).cast(\"double\"))\\\n",
    "                .withColumn(\"mode\", split(songs_df2['values'], '~').getItem(13).cast(\"int\"))\\\n",
    "                .withColumn(\"speechiness\", split(songs_df2['values'], '~').getItem(14).cast(\"double\"))\\\n",
    "                .withColumn(\"acousticness\", split(songs_df2['values'], '~').getItem(15).cast(\"double\"))\\\n",
    "                .withColumn(\"instrumentalness\", split(songs_df2['values'], '~').getItem(16).cast(\"double\"))\\\n",
    "                .withColumn(\"liveness\", split(songs_df2['values'], '~').getItem(17).cast(\"double\"))\\\n",
    "                .withColumn(\"valence\", split(songs_df2['values'], '~').getItem(18).cast(\"double\"))\\\n",
    "                .withColumn(\"tempo\", split(songs_df2['values'], '~').getItem(19).cast(\"double\"))\\\n",
    "                .withColumn(\"time_signature\", split(songs_df2['values'], '~').getItem(20).cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57c7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9651 entries, 0 to 9650\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   value             9651 non-null   object \n",
      " 1   values            9651 non-null   object \n",
      " 2   order_id          9651 non-null   int32  \n",
      " 3   id                9651 non-null   object \n",
      " 4   name              9651 non-null   object \n",
      " 5   popularity        9651 non-null   int32  \n",
      " 6   duration_ms       9651 non-null   float64\n",
      " 7   explicit          9651 non-null   int32  \n",
      " 8   artists           9651 non-null   object \n",
      " 9   id_artists        9651 non-null   object \n",
      " 10  release_date      9651 non-null   object \n",
      " 11  danceability      9651 non-null   float64\n",
      " 12  energy            9651 non-null   float64\n",
      " 13  key               9651 non-null   int32  \n",
      " 14  loudness          9651 non-null   float64\n",
      " 15  mode              9651 non-null   int32  \n",
      " 16  speechiness       9651 non-null   float64\n",
      " 17  acousticness      9651 non-null   float64\n",
      " 18  instrumentalness  9651 non-null   float64\n",
      " 19  liveness          9651 non-null   float64\n",
      " 20  valence           9651 non-null   float64\n",
      " 21  tempo             9651 non-null   float64\n",
      " 22  time_signature    9651 non-null   float64\n",
      "dtypes: float64(11), int32(5), object(7)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "songs_df3.toPandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f617ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tracks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson\n",
    "import spotipy\n",
    "import spotipy.util\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ[\"SPOTIPY_CLIENT_ID\"] = 'a997751eb4be4e87bb3a0a246b0a4b1b'\n",
    "os.environ[\"SPOTIPY_CLIENT_SECRET\"] = '4ee43f4a92fd4bd882fe2667d82ccf50'\n",
    "os.environ[\"SPOTIPY_REDIRECT_URI\"] = 'http://localhost:5000/callback'\n",
    "\n",
    "scope = 'user-library-read'\n",
    "username = 'Tin Pham'\n",
    "\n",
    "token = spotipy.util.prompt_for_user_token(username, scope)\n",
    "\n",
    "if token:\n",
    "    spotipy_obj = spotipy.Spotify(auth=token)\n",
    "    saved_tracks_resp = spotipy_obj.current_user_saved_tracks(limit=50)\n",
    "else:\n",
    "    print('Couldn\\'t get token for that username')\n",
    "    \n",
    "number_of_tracks = saved_tracks_resp['total']\n",
    "print('%d tracks' % number_of_tracks)\n",
    "\n",
    "def save_only_some_fields(track_response):\n",
    "    return {        \n",
    "        'id': str(track_response['track']['id']),\n",
    "        'name': str(track_response['track']['name']),\n",
    "        'artists': [artist['name'] for artist in track_response['track']['artists']],\n",
    "        'duration_ms': track_response['track']['duration_ms'],\n",
    "        'popularity': track_response['track']['popularity'],\n",
    "        'added_at': track_response['added_at']\n",
    "    }\n",
    "\n",
    "tracks = [save_only_some_fields(track) for track in saved_tracks_resp['items']]\n",
    "\n",
    "while saved_tracks_resp['next']:\n",
    "    saved_tracks_resp = spotipy_obj.next(saved_tracks_resp)\n",
    "    tracks.extend([save_only_some_fields(track) for track in saved_tracks_resp['items']])\n",
    "\n",
    "\n",
    "tracks_df = pd.DataFrame(tracks)\n",
    "pd.set_option('display.max_rows', len(tracks))\n",
    "\n",
    "\n",
    "tracks_df['artists'] = tracks_df['artists'].apply(lambda artists: artists[0])\n",
    "tracks_df['duration_ms'] = tracks_df['duration_ms'].apply(lambda duration: duration/1000)\n",
    "\n",
    "tracks_df = tracks_df.rename(columns = {'duration_ms':'duration_s'})\n",
    "\n",
    "\n",
    "audio_features = {}\n",
    "\n",
    "for idd in tracks_df['id'].tolist():\n",
    "    audio_features[idd] = spotipy_obj.audio_features(idd)[0]\n",
    "    \n",
    "tracks_df['acousticness'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['acousticness'])\n",
    "tracks_df['speechiness'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['speechiness'])\n",
    "tracks_df['key'] = tracks_df['id'].apply(lambda idd: str(audio_features[idd]['key']))\n",
    "tracks_df['liveness'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['liveness'])\n",
    "tracks_df['instrumentalness'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['instrumentalness'])\n",
    "tracks_df['energy'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['energy'])\n",
    "tracks_df['tempo'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['tempo'])\n",
    "tracks_df['time_signature'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['time_signature'])\n",
    "tracks_df['loudness'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['loudness'])\n",
    "tracks_df['danceability'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['danceability'])\n",
    "tracks_df['valence'] = tracks_df['id'].apply(lambda idd: audio_features[idd]['valence'])\n",
    "import pandas as pd\n",
    "\n",
    "song_data = tracks_df\n",
    "#song_data.rename(columns={'duration_s': 'duration_ms' }, inplace=True)\n",
    "song_data = song_data.drop(['id', 'added_at', 'time_signature','duration_s'], axis='columns')\n",
    "add_df = song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9faf9695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = songs_df3\n",
    "\n",
    "\n",
    "df_stream = df\n",
    "\n",
    "df = df.drop('value','values','order_id',\n",
    " 'id',\n",
    " 'explicit',\n",
    "  'mode',\n",
    " 'release_date',\n",
    " 'id_artists',\n",
    " 'time_signature',\n",
    " 'duration_ms',\n",
    " 'timestamp')\n",
    "\n",
    "df_sp = spark.createDataFrame(add_df)\n",
    "df = df.union(df_sp)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler=VectorAssembler(inputCols=[\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'loudness',\n",
    " 'speechiness',\n",
    " 'acousticness',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'valence',\n",
    " 'tempo'], outputCol='features')\n",
    "assembled_data=assembler.setHandleInvalid(\"skip\").transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a52afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "data_scale=scale.fit(assembled_data)\n",
    "df=data_scale.transform(assembled_data)\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "silhouette_score=[]\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "\n",
    "\n",
    "KMeans_algo=KMeans(featuresCol='standardized', k=3)\n",
    "    \n",
    "KMeans_fit=KMeans_algo.fit(df)\n",
    "    \n",
    "output_df =KMeans_fit.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a31a409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class SpotifyRecommender():\n",
    "    def __init__(self, rec_data):\n",
    "        self.rec_data_ = rec_data\n",
    "    \n",
    "    def spotify_recommendations(self, song_name, amount=1):\n",
    "        distances = []\n",
    "        song = self.rec_data_[(self.rec_data_.name.str.lower() == song_name.lower())].head(1).values[0]\n",
    "        # get details of our fav song from name we pass as x earlier.\n",
    "        res_data = self.rec_data_[self.rec_data_.name.str.lower() != song_name.lower()]\n",
    "        #dropping the data with our fav song so that it doesnt affect our recommendation.\n",
    "        for r_song in tqdm(res_data.values):\n",
    "            # tqdm is just used for showing the bar of iteration through our streamed songs.\n",
    "            dist = 0\n",
    "            for col in np.arange(len(res_data.columns)):\n",
    "                # (len(res_data.columns) gets us the number of columns -> 13 in our case.\n",
    "                #indeces of non-numerical columns neednt be considered.\n",
    "                if not col in [0,1,13]:\n",
    "                    #calculating the manhettan distances for each numerical feature\n",
    "                    # song -> from our fav dataset.\n",
    "                    # r_song -> from streaming data.\n",
    "                \n",
    "                    dist = dist + np.absolute(float(song[col]) - float(r_song[col]))\n",
    "            distances.append(dist)\n",
    "            # distances are calculated and appended and added to a new column called distances in our dataset.\n",
    "        res_data['distance'] = distances\n",
    "        #sorting our data to be ascending by 'distance' feature\n",
    "        res_data = res_data.sort_values('distance')\n",
    "        # resulting dataset have the song similar to our fav song's numerical values and thus recommended.\n",
    "        columns = ['name', 'artists', 'acousticness', 'liveness', 'instrumentalness', 'energy', 'danceability', 'valence']\n",
    "        return res_data[columns][:amount]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0213464",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Từ Ngày Em Đến\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4074/4074 [00:00<00:00, 28716.76it/s]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "datad = output_df.select('name',\n",
    " 'artists',\n",
    " 'danceability',\n",
    " 'energy',\n",
    " 'key',\n",
    " 'loudness',\n",
    " 'speechiness',\n",
    " 'acousticness',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'valence',\n",
    " 'tempo',\n",
    " 'prediction')\n",
    "\n",
    "\n",
    "\n",
    "datf = datad.toPandas()\n",
    "datf.drop(datf[datf['artists'] == '0'].index, inplace = True)\n",
    "datf.drop_duplicates(inplace=True)\n",
    "datf.drop(datf[datf['danceability'] == 0.0000].index, inplace = True)\n",
    "datf.drop(datf[datf['liveness'] == 0.000].index, inplace = True)\n",
    "datf.drop(datf[datf['instrumentalness'] == 0.000000].index, inplace = True)\n",
    "datf.drop(datf[datf['energy'] == 0.0000].index, inplace = True)\n",
    "datf.drop(datf[datf['danceability'] == 0.000].index, inplace = True)\n",
    "datf.drop(datf[datf['valence'] == 0.000].index, inplace = True)\n",
    "\n",
    "# y = datf\n",
    "# value_pred = datf.iloc[-1:]['prediction']\n",
    "#datf = datf[datf['prediction'] == list(value_pred)[0]]\n",
    "recommender = SpotifyRecommender(datf)\n",
    "x = add_df['name'].tolist()[0]\n",
    "print(x)\n",
    "rec_song = recommender.spotify_recommendations(str(x), 5)\n",
    "\n",
    "v = add_df[['name', 'artists',  'acousticness', 'liveness', 'instrumentalness', 'energy', \n",
    "       'danceability', 'valence']]\n",
    "\n",
    "rec_song = pd.concat([rec_song, v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbcee1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------+----------------+------+------------+-------+\n",
      "|                name|             artists|acousticness|liveness|instrumentalness|energy|danceability|valence|\n",
      "+--------------------+--------------------+------------+--------+----------------+------+------------+-------+\n",
      "|      Sativa - Remix|   ['DJ Alan Gomez']|       0.152|   0.175|         2.46E-4| 0.621|       0.739|  0.618|\n",
      "|             Oh Love|  ['Emilie Nicolas']|       0.892|  0.0866|         1.31E-6| 0.105|       0.415|  0.323|\n",
      "|Na Ponta do P\\u00...|      ['Mc Livinho']|       0.472|  0.0906|         1.05E-6|  0.76|       0.592|  0.634|\n",
      "|             Element|       ['Pop Smoke']|      0.0301|   0.251|         2.18E-6| 0.878|       0.772|  0.305|\n",
      "|Hoy Se Chicha - R...|['DJ Alan Gomez',...|      0.0201|   0.155|         4.95E-5| 0.638|       0.516|   0.61|\n",
      "|      Từ Ngày Em Đến|              Da LAB|       0.418|   0.105|             0.0|  0.69|       0.699|  0.637|\n",
      "+--------------------+--------------------+------------+--------+----------------+------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rec = spark.createDataFrame(rec_song)\n",
    "df_rec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db1eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
